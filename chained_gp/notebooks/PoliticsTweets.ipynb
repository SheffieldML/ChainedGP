{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import GPy\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import io\n",
    "\n",
    "import pandas as pd\n",
    "from GPy.plotting.matplot_dep.util import fixed_inputs\n",
    "pylab.rcParams['figure.figsize'] = 10, 8  # that's default image size for this \n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "home_dir = \"~/Work/Code/het_nongauss/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tweet_download.py to download the tweets, then politics_parse.py to generate the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load=True\n",
    "API = True\n",
    "NLTK = False\n",
    "#Get the text, analyse the probabilities, and put into their associated columns\n",
    "if NLTK:\n",
    "    if load:\n",
    "        ukip_df = pd.read_csv('ukip_nltk.csv')\n",
    "        greens_df = pd.read_csv('greens_nltk.csv')\n",
    "elif API:\n",
    "    if load:\n",
    "        ukip_df = pd.read_csv('../data_download/twitter_data/twitter_data/parsed/ukip_API.csv')\n",
    "        greens_df = pd.read_csv('../data_download/twitter_data/twitter_data/parsed/greens_API.csv')\n",
    "        conservative_df = pd.read_csv('../data_download/twitter_data/twitter_data/parsed/conservative_API.csv')\n",
    "        labour_df = pd.read_csv('../data_download/twitter_data/twitter_data/parsed/labour_API.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = [(ukip_df, 'ukip', 'm'), (greens_df, 'greens', 'g'),\n",
    "       (conservative_df, 'conservative', 'b'), (labour_df, 'labour', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge all the dataframes\n",
    "ukip_df['party'] = 'ukip'\n",
    "greens_df['party'] = 'greens'\n",
    "conservative_df['party'] = 'conservative'\n",
    "labour_df['party'] = 'labour'\n",
    "\n",
    "ukip_df['color'] = 'm'\n",
    "greens_df['color'] = 'g'\n",
    "conservative_df['color'] = 'b'\n",
    "labour_df['color'] = 'r'\n",
    "\n",
    "tweets_df = pd.concat([ukip_df, greens_df, conservative_df, labour_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all that are exactly 50/50 as these come from not being able to find any useful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['party'] == 'ukip'].reset_index().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.dates import date2num\n",
    "tweets_df.ix[tweets_df['pos'] == 0.5, 'pos'] = np.nan\n",
    "tweets_df.dropna(subset=['pos'], inplace=True)\n",
    "tweets_df['time'] = pd.to_datetime(tweets_df['time'])\n",
    "#Make timestamp from time object\n",
    "tweets_df['timestamp'] = tweets_df['time'].apply(date2num)\n",
    "parties_df = tweets_df.groupby('party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parties = tweets_df['party'].unique()\n",
    "num_parties = parties.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labour_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['party'] == 'ukip']['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['party'] == 'labour']['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['party'] == 'labour'].reindex().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['party'] == 'ukip'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['party'] == 'labour'][['id_str', 'pos', 'neg']].to_csv('labour_raw_ids.csv', index=False)\n",
    "tweets_df[tweets_df['party'] == 'conservative'][['id_str', 'pos', 'neg']].to_csv('conservative_raw_ids.csv', index=False)\n",
    "tweets_df[tweets_df['party'] == 'greens'][['id_str', 'pos', 'neg']].to_csv('greens_raw_ids.csv', index=False)\n",
    "tweets_df[tweets_df['party'] == 'ukip'][['id_str', 'pos', 'neg']].to_csv('ukip_raw_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(parties), 1, sharex=True)\n",
    "fig_hist, hist_ax = plt.subplots(1,1)\n",
    "for (party, party_df), ax in zip(parties_df, axes):\n",
    "    print party\n",
    "    color = party_df['color'][0]\n",
    "    \n",
    "    party_df['pos'].plot(kind='hist', alpha=0.49, normed=True, label=party, ax=hist_ax, color=color)\n",
    "    plt.title(\"Distribution of tweets positiveness\")\n",
    "\n",
    "    ax.set_title(\"{} positiveness over time\".format(party))\n",
    "    party_df.plot(x='time', y='pos', ax=ax, c=color, label=party, legend=False, alpha=0.7)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlim(tweets_df['timestamp'].min(), tweets_df['timestamp'].max())\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(dfs), 1, sharex=True)\n",
    "for (party, party_df), ax in zip(parties_df, axes):\n",
    "    color = party_df['color'][0]\n",
    "    ax.set_title(\"{} positiveness over time\".format(party))\n",
    "    ax.plot_date(party_df['timestamp'], party_df['pos'], 'o', color=color, alpha=0.01, lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are very centred, so we can transform them to see some more interesting shapes in our beta distribution, like reweighting exam scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort based on 'pos' with all parties together\n",
    "transformed_tweets = tweets_df.sort(columns='pos')\n",
    "\n",
    "#draw N random uniform 'pos' values\n",
    "#replace each pos value with its corresponding (index) random value\n",
    "transformed_tweets['pos'] = np.sort(np.random.uniform(0, 1, transformed_tweets.shape[0]))\n",
    "\n",
    "transformed_grouped = transformed_tweets.groupby('party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(parties), 1, sharex=True)\n",
    "fig_hist, hist_ax = plt.subplots(1,1)\n",
    "for (party, party_df), ax in zip(transformed_grouped, axes):\n",
    "    print party\n",
    "    color = party_df['color'][0]\n",
    "    \n",
    "    party_df['pos'].plot(kind='hist', alpha=0.49, normed=True, label=party, ax=hist_ax, color=color)\n",
    "    plt.title(\"Distribution of transformed tweets positiveness\")\n",
    "\n",
    "    ax.set_title(\"{} positiveness over time\".format(party))\n",
    "    party_df.plot(x='time', y='pos', ax=ax, c=color, label=party, legend=False, alpha=0.7)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlim(tweets_df['timestamp'].min(), tweets_df['timestamp'].max())\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(dfs), 1, sharex=True)\n",
    "for (party, party_df), ax in zip(transformed_grouped, axes):\n",
    "    color = party_df['color'][0]\n",
    "    ax.set_title(\"{} positiveness over time\".format(party))\n",
    "    ax.plot_date(party_df['timestamp'], party_df['pos'], 'o', color=color, alpha=0.01, lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit a GP to it, with a beta likelihood and latent functions over log of both parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "party = 'labour'\n",
    "party_df = parties_df.get_group(party)\n",
    "#party_df = transformed_grouped.get_group(party)\n",
    "party_df = party_df.sort(columns='pos')\n",
    "party_df = party_df.reindex(np.random.permutation(party_df.index))\n",
    "color = party_df['color'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subsample = 5000\n",
    "#if subsample is not None:\n",
    "    #df = df.loc[np.random.choice(df.index, subsample, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = party_df['timestamp'][:, None]\n",
    "X_offset = X.mean()\n",
    "X_scale = X.max() - X.min()\n",
    "X = (X - X_offset)/X_scale\n",
    "Z = np.linspace(X.min(), X.max(), 100)[:, None]\n",
    "Y = party_df['pos'][:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data in GPy form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from het_beta import HetBeta\n",
    "from svgp_beta import SVGPBeta\n",
    "import GPy\n",
    "\n",
    "likelihood = HetBeta()\n",
    "\n",
    "kernf = GPy.kern.Matern32(1, lengthscale=0.3, name='kernf_rbf1')\n",
    "kernf += GPy.kern.White(1, variance=1e-5, name='f_white')\n",
    "#kernf += GPy.kern.RBF(1, lengthscale=0.6, name='kernf_rbf2')\n",
    "#Needs white or variance doesn't checkgrad!\n",
    "kerng = GPy.kern.Matern32(1, lengthscale=0.3, name='kerng_rbf1')\n",
    "kerng += GPy.kern.White(1, variance=1e-5, name='g_white')\n",
    "#kerng += GPy.kern.RBF(1, lengthscale=0.6, name='kerng_rbf2')\n",
    "kernf.name = 'kernf'\n",
    "kerng.name = 'kerng'\n",
    "\n",
    "kern = [kernf, kerng]\n",
    "\n",
    "m = SVGPBeta(X, Y, Z, kern, likelihood, batchsize=500)\n",
    "\n",
    "m.kernf.f_white.fix()\n",
    "m.kerng.g_white.fix()\n",
    "m.kernf.fix()\n",
    "m.kerng.fix()\n",
    "m.Z.fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import climin\n",
    "import sys\n",
    "opt = climin.Adadelta(m.optimizer_array, m.stochastic_grad, step_rate=0.1)\n",
    "#opt = climin.RmsProp(m.optimizer_array, m.stochastic_grad, step_rate=0.04)\n",
    "\n",
    "def callback(i, max_iter=5000):\n",
    "    ll = m.log_likelihood()\n",
    "    print str(i['n_iter']) + \" \" + str(ll) + \" \" + str(np.max(i['gradient'])), \"\\r\",\n",
    "    if np.isnan(ll):\n",
    "        raise ValueError('Log likelihood went to nan')\n",
    "    #Stop after max_iter iterations\n",
    "    if i['n_iter'] > max_iter:\n",
    "        return True\n",
    "    return False\n",
    "from functools import partial\n",
    "c_init = partial(callback, max_iter=1000)\n",
    "info = opt.minimize_until(c_init)\n",
    "\n",
    "m.kernf.constrain_positive()\n",
    "m.kerng.constrain_positive()\n",
    "opt = climin.Adadelta(m.optimizer_array, m.stochastic_grad, step_rate=0.03)\n",
    "c_full = partial(callback, max_iter=500)\n",
    "info = opt.minimize_until(c_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = climin.Adadelta(m.optimizer_array, m.stochastic_grad, step_rate=0.01)\n",
    "c_full = partial(callback, max_iter=100)\n",
    "info = opt.minimize_until(c_full)\n",
    "opt = climin.Adadelta(m.optimizer_array, m.stochastic_grad, step_rate=0.001)\n",
    "c_full = partial(callback, max_iter=100)\n",
    "info = opt.minimize_until(c_full)\n",
    "opt = climin.Adadelta(m.optimizer_array, m.stochastic_grad, step_rate=0.0001)\n",
    "c_full = partial(callback, max_iter=100)\n",
    "info = opt.minimize_until(c_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from GPy.plotting.matplot_dep.models_plots import fixed_inputs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta as beta_dist\n",
    "from svgp_multi import SVGPMulti\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import num2date\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def plot_fs(self, dim=0, variances=False, median=True, true_variance=True,\n",
    "                y_alpha=0.3, cmap=plt.cm.YlOrRd, num_pred_points=200,\n",
    "                X_scale=1.0, X_offset=0.0, plot_dates=True, subsample=True):\n",
    "    \"\"\"\n",
    "    Plotting for models with two latent functions, one is an exponent over the scale\n",
    "    parameter\n",
    "    \"\"\"\n",
    "    assert self.likelihood.request_num_latent_functions(self.Y) == 2\n",
    "    if median:\n",
    "        XX = fixed_inputs(self, non_fixed_inputs=[dim], fix_routine='median', as_list=False, X_all=True)\n",
    "    else:\n",
    "        XX = fixed_inputs(self, non_fixed_inputs=[dim], fix_routine='mean', as_list=False, X_all=True)\n",
    "    #Now we have the other values fixed, remake the matrix to be the right shape\n",
    "    XX = np.zeros((num_pred_points, self.X_all.shape[1]))\n",
    "    for d in range(self.X_all.shape[1]):\n",
    "        XX[:, d] = self.X_all[0, d]\n",
    "    X_pred_points = XX.copy()\n",
    "    X_pred_points_lin = np.linspace(self.X_all[:, dim].min(), self.X_all[:, dim].max(), XX.shape[0])\n",
    "    X_pred_points[:, dim] = X_pred_points_lin\n",
    "\n",
    "    mf, covf = self._raw_predict(X_pred_points, 0, full_cov=True)\n",
    "    mg, covg = self._raw_predict(X_pred_points, 1, full_cov=True)\n",
    "\n",
    "    covf = covf[:,:,0]\n",
    "    covg = covg[:,:,0]\n",
    "\n",
    "    num_samples = 60\n",
    "    samples_f = np.random.multivariate_normal(mf.flatten(), covf, num_samples)\n",
    "    samples_g = np.random.multivariate_normal(mg.flatten(), covg, num_samples)\n",
    "\n",
    "    alpha = np.exp(samples_f)\n",
    "    beta = np.exp(samples_g)\n",
    "\n",
    "    num_y_pixels = 60\n",
    "    #Want the top left pixel to be evaluated at 1\n",
    "    line = np.linspace(1, 0, num_y_pixels)\n",
    "    res = np.zeros((X_pred_points.shape[0], num_y_pixels))\n",
    "    for j in range(X_pred_points.shape[0]):\n",
    "        sf = alpha[:, j]  # Pick out the jth point along X axis\n",
    "        sg = beta[:, j]\n",
    "        for i in range(num_samples):\n",
    "            # Pick out the sample and evaluate the pdf on a line between 0\n",
    "            # and 1 with these alpha and beta values\n",
    "            res[j, :] += beta_dist.pdf(line, sf[i], sg[i])\n",
    "        res[j, :] /= num_samples\n",
    "\n",
    "    vmax, vmin = res[np.isfinite(res)].max(), res[np.isfinite(res)].min()\n",
    "    \n",
    "    import matplotlib.colors\n",
    "    norm = matplotlib.colors.Normalize(vmax=vmax, vmin=vmin)\n",
    "    \n",
    "    X_all = self.X_all*X_scale + X_offset\n",
    "    X_pred_points = X_pred_points*X_scale + X_offset\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, sharex=True)\n",
    "    plt.tight_layout(pad=1.0, w_pad=0.5, h_pad=2.0)\n",
    "    ax1.set_title('averaged pdf and data')\n",
    "    im = ax1.imshow(res.T, origin='upper', \n",
    "                    extent=[X_pred_points[:,dim].min(),X_pred_points[:,dim].max(), 0, 1],\n",
    "                    aspect='auto', cmap=cmap, norm=norm)\n",
    "    fig.colorbar(im, orientation='horizontal', pad=0.2)\n",
    "    #Subsample and change y_alpha accordingly\n",
    "    subsample_inds = np.random.permutation(range(X_all.shape[0]))[:int(X_all.shape[0]*subsample)]\n",
    "    X_sub = X_all[subsample_inds, :]\n",
    "    Y_sub = self.Y_all[subsample_inds, :]\n",
    "    y_alpha = y_alpha/float(subsample)\n",
    "    if plot_dates:\n",
    "        #All others should follow suit since we sharex\n",
    "        ax1.plot_date(X_sub, Y_sub, 'kx', alpha=y_alpha)\n",
    "    else:\n",
    "        ax1.plot(X_sub, Y_sub, 'kx', alpha=y_alpha)\n",
    "\n",
    "    #For labels\n",
    "    ax2.set_title('Posterior GP for Beta distributed variables')\n",
    "    ax2.plot(X_pred_points, beta.T[:,0], 'b-', label='beta', alpha=3./num_samples)\n",
    "    ax2.plot(X_pred_points, alpha.T[:,0], 'm-', label='alpha', alpha=3./num_samples)\n",
    "    \n",
    "    #For rest of samples\n",
    "    ax2.plot(X_pred_points, beta.T[:,1:], 'b-', alpha=3./num_samples)\n",
    "    ax2.plot(X_pred_points, alpha.T[:,1:], 'm-', alpha=3./num_samples)\n",
    "    ax2.legend()\n",
    "    \n",
    "    ax3.plot(X_pred_points, alpha.T / (alpha.T + beta.T), 'b-', alpha=3./num_samples)\n",
    "    ax3.set_title('Mean')\n",
    "\n",
    "    var = (alpha.T*beta.T) / ((alpha.T + beta.T)**2 * (alpha.T+beta.T +1))\n",
    "    ax4.plot(X_pred_points, var, 'b-', alpha=3./num_samples)\n",
    "    ax4.set_title('variance')\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        a = alpha[i, :]\n",
    "        b = beta[i, :]\n",
    "        mode = (a - 1) / (a + b - 2)\n",
    "        mode = np.where(mode < 0, np.nan, mode)\n",
    "        ax5.plot(X_pred_points, mode, 'b-', alpha=3./num_samples)\n",
    "    ax5.set_title('Modes where they exist (alpha > 1, beta > 1)')\n",
    "    ax5.set_ylim(0,1)\n",
    "    plt.legend()\n",
    "\n",
    "    ax1.set_xlim(X_pred_points[:, dim].min(), X_pred_points[:, dim].max())\n",
    "\n",
    "    fig3d = plt.figure(figsize=(13,5))\n",
    "    ax = fig3d.add_subplot(111, projection='3d')\n",
    "    ax.view_init(elev=55., azim=300.0)\n",
    "    axlim_min, axlim_max = X_pred_points[:, dim].min(), X_pred_points[:, dim].max()\n",
    "    x, y = np.mgrid[axlim_min:axlim_max:complex(res.shape[0]),\n",
    "                    1:0:complex(res.shape[1])]\n",
    "    #x_dates = num2date(x)\n",
    "    xfmt = mdates.DateFormatter('%b %d')\n",
    "    ax.plot_surface(x,y,res,cmap=cmap,rstride=1, cstride=1, lw=0.05, alpha=1, edgecolor='b', norm=norm)\n",
    "    #ax.xaxis.set_major_formatter(xfmt)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%Y'))\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax.set_zlabel('PDF')\n",
    "    ax.set_ylabel('Sentiment')\n",
    "    ax.set_xlabel('Date')\n",
    "    #ax.autofmt_xdate()\n",
    "    \n",
    "    return fig, fig3d\n",
    "\n",
    "from functools import partial\n",
    "m.plot_fs1 = partial(plot_fs, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.plot_fs1(X_scale=X_scale, X_offset=X_offset, y_alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "palette[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rgb2hex(rgb):\n",
    "    def clamp(x): \n",
    "        return max(0, min(int(x), 255))\n",
    "    return \"#{0:02x}{1:02x}{2:02x}\".format(clamp(rgb[0]), clamp(rgb[1]), clamp(rgb[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from GPy.plotting.matplot_dep.models_plots import fixed_inputs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta as beta_dist\n",
    "from svgp_multi import SVGPMulti\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import num2date\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "def save_plot_fs(self, dim=0, variances=False, median=True, true_variance=True,\n",
    "                y_alpha=0.3, cmap=plt.cm.YlOrRd, num_pred_points=200,\n",
    "                X_scale=1.0, X_offset=0.0, plot_dates=True, subsample=1.0):\n",
    "    \"\"\"\n",
    "    Plotting for models with two latent functions, one is an exponent over the scale\n",
    "    parameter\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    #sns.set_style(style='white')\n",
    "    #palette = sns.color_palette(\"hls\")\n",
    "    assert self.likelihood.request_num_latent_functions(self.Y) == 2\n",
    "    subsample = float(subsample)\n",
    "    if median:\n",
    "        XX = fixed_inputs(self, non_fixed_inputs=[dim], fix_routine='median', as_list=False, X_all=True)\n",
    "    else:\n",
    "        XX = fixed_inputs(self, non_fixed_inputs=[dim], fix_routine='mean', as_list=False, X_all=True)\n",
    "    #Now we have the other values fixed, remake the matrix to be the right shape\n",
    "    XX = np.zeros((num_pred_points, self.X_all.shape[1]))\n",
    "    for d in range(self.X_all.shape[1]):\n",
    "        XX[:, d] = self.X_all[0, d]\n",
    "    X_pred_points = XX.copy()\n",
    "    X_pred_points_lin = np.linspace(self.X_all[:, dim].min(), self.X_all[:, dim].max(), XX.shape[0])\n",
    "    X_pred_points[:, dim] = X_pred_points_lin\n",
    "\n",
    "    mf, covf = self._raw_predict(X_pred_points, 0, full_cov=True)\n",
    "    mg, covg = self._raw_predict(X_pred_points, 1, full_cov=True)\n",
    "\n",
    "    covf = covf[:,:,0]\n",
    "    covg = covg[:,:,0]\n",
    "\n",
    "    num_samples = 30\n",
    "    samples_f = np.random.multivariate_normal(mf.flatten(), covf, num_samples)\n",
    "    samples_g = np.random.multivariate_normal(mg.flatten(), covg, num_samples)\n",
    "\n",
    "    alpha = np.exp(samples_f)\n",
    "    beta = np.exp(samples_g)\n",
    "\n",
    "    num_y_pixels = 40\n",
    "    #Want the top left pixel to be evaluated at 1\n",
    "    line = np.linspace(1, 0, num_y_pixels)\n",
    "    res = np.zeros((X_pred_points.shape[0], num_y_pixels))\n",
    "    for j in range(X_pred_points.shape[0]):\n",
    "        sf = alpha[:, j]  # Pick out the jth point along X axis\n",
    "        sg = beta[:, j]\n",
    "        for i in range(num_samples):\n",
    "            # Pick out the sample and evaluate the pdf on a line between 0\n",
    "            # and 1 with these alpha and beta values\n",
    "            res[j, :] += beta_dist.pdf(line, sf[i], sg[i])\n",
    "        res[j, :] /= num_samples\n",
    "\n",
    "    vmax, vmin = res[np.isfinite(res)].max(), res[np.isfinite(res)].min()\n",
    "    \n",
    "    import matplotlib.colors\n",
    "    norm = matplotlib.colors.Normalize(vmax=vmax, vmin=vmin)\n",
    "    \n",
    "    X_all = self.X_all*X_scale + X_offset\n",
    "    X_pred_points = X_pred_points*X_scale + X_offset\n",
    "    fig_data, ax1 = plt.subplots(1)\n",
    "    fig_latents, ax2 = plt.subplots(1)\n",
    "    fig_mean, ax3 = plt.subplots(1)\n",
    "    fig_var, ax4 = plt.subplots(1)\n",
    "    fig_modes, ax5 = plt.subplots(1)\n",
    "    fig_mean_var, ax6 = plt.subplots(1)\n",
    "    fig3d = plt.figure(figsize=(8,4))\n",
    "    \n",
    "    fig_data.set_figwidth(12)\n",
    "    fig_data.set_figheight(2)\n",
    "    fig_latents.set_figwidth(5)\n",
    "    fig_latents.set_figheight(2)\n",
    "    fig_mean.set_figwidth(5)\n",
    "    fig_mean.set_figheight(2)\n",
    "    fig_var.set_figwidth(5)\n",
    "    fig_var.set_figheight(2)\n",
    "    fig_modes.set_figwidth(5)\n",
    "    fig_modes.set_figheight(2)\n",
    "    fig_mean_var.set_figwidth(5)\n",
    "    fig_mean_var.set_figheight(2)\n",
    "    \n",
    "    #ax1.set_title('Beta PDF')\n",
    "    im = ax1.imshow(res.T, origin='upper', \n",
    "                    extent=[X_pred_points[:,dim].min(),X_pred_points[:,dim].max(), 0, 1],\n",
    "                    aspect='auto', cmap=cmap, norm=norm)\n",
    "    fig.colorbar(im, orientation='horizontal', pad=0.2)\n",
    "    #Subsample and change y_alpha accordingly\n",
    "    subsample_inds = np.random.permutation(range(X_all.shape[0]))[:int(X_all.shape[0]*subsample)]\n",
    "    X_sub = X_all[subsample_inds, :]\n",
    "    Y_sub = self.Y_all[subsample_inds, :]\n",
    "    y_alpha = y_alpha/float(subsample)\n",
    "    if plot_dates:\n",
    "        #All others should follow suit since we sharex\n",
    "        ax1.plot_date(X_sub, Y_sub, 'kx', alpha=y_alpha)\n",
    "    else:\n",
    "        ax1.plot(X_sub, Y_sub, 'kx', alpha=y_alpha)\n",
    "    ax1.set_ylabel('Sentiment')\n",
    "    \n",
    "    #For labels\n",
    "    ax2.set_title(r'Posterior GP for latent functions of $\\textrm{Beta}(\\alpha,\\beta)$')\n",
    "    ax2.plot(X_pred_points, beta.T[:,0], 'b-', label=r'$\\beta$', alpha=3./num_samples)\n",
    "    ax2.plot(X_pred_points, alpha.T[:,0], 'm-', label=r'$\\alpha$', alpha=3./num_samples)\n",
    "    \n",
    "    #For rest of samples\n",
    "    ax2.plot(X_pred_points, beta.T[:,1:], 'b-', alpha=3./num_samples)\n",
    "    ax2.plot(X_pred_points, alpha.T[:,1:], 'm-', alpha=3./num_samples)\n",
    "\n",
    "    \n",
    "    ax3.plot(X_pred_points, alpha.T / (alpha.T + beta.T), 'b-', alpha=3./num_samples)\n",
    "    ax3.set_title('Mean')\n",
    "\n",
    "    var = (alpha.T*beta.T) / ((alpha.T + beta.T)**2 * (alpha.T+beta.T +1))\n",
    "    ax4.plot(X_pred_points, var, 'b-', alpha=3./num_samples)\n",
    "    ax4.set_title('Variance')\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        a = alpha[i, :]\n",
    "        b = beta[i, :]\n",
    "        mode = (a - 1) / (a + b - 2)\n",
    "        mode = np.where(mode < 0, np.nan, mode)\n",
    "        ax5.plot(X_pred_points, mode, 'b-', alpha=3./num_samples)\n",
    "    ax5.set_title(r'Modes where they exist ($\\alpha > 1$, $\\beta > 1$)', fontsize=10)\n",
    "    ax5.set_ylim(0,1)\n",
    "\n",
    "\n",
    "    ax = fig3d.add_subplot(111, projection='3d')\n",
    "    ax.view_init(elev=55., azim=300.0)\n",
    "    axlim_min, axlim_max = X_pred_points[:, dim].min(), X_pred_points[:, dim].max()\n",
    "    x, y = np.mgrid[axlim_min:axlim_max:complex(res.shape[0]),\n",
    "                    1:0:complex(res.shape[1])]\n",
    "    #x_dates = num2date(x)\n",
    "    xfmt = mdates.DateFormatter('%b %d')\n",
    "    xfmt = mdates.DateFormatter('%m/%d/%y')\n",
    "    p = ax.plot_surface(x,y,res,cmap=cmap,rstride=1, cstride=1, lw=0.01, alpha=1, edgecolor='b', norm=norm)\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax.set_zlabel('PDF')\n",
    "    ax.set_ylabel('Sentiment')\n",
    "\n",
    "    #plt.colorbar(p, orientation='vertical', pad=0.1)\n",
    "    \n",
    "    #Twin plot the mean and variance\n",
    "    ax6.plot(X_pred_points, alpha.T / (alpha.T + beta.T), 'b-', alpha=3./num_samples)\n",
    "    for tl in ax6.get_yticklabels():\n",
    "        tl.set_color('b')\n",
    "    ax6.set_ylabel('Mean', color='b')\n",
    "    ax6.set_title('Mean and Variance of Beta($\\alpha$, $\\beta$)')\n",
    "    \n",
    "    ax7 = ax6.twinx()\n",
    "    var = (alpha.T*beta.T) / ((alpha.T + beta.T)**2 * (alpha.T+beta.T +1))\n",
    "    ax7.plot(X_pred_points, var, 'm-', alpha=3./num_samples)\n",
    "    for tl in ax7.get_yticklabels():\n",
    "        tl.set_color('m')\n",
    "    ax7.set_ylabel('Variance', color='m')\n",
    "    ax6.grid(True)\n",
    "    \n",
    "    ax2.legend(loc='lower right', bbox_to_anchor=(1.15, 0.7))\n",
    "    ax1.set_xlim(X_pred_points[:, dim].min(), X_pred_points[:, dim].max())\n",
    "    ax2.set_xlim(X_pred_points[:, dim].min(), X_pred_points[:, dim].max())\n",
    "    ax3.set_xlim(X_pred_points[:, dim].min(), X_pred_points[:, dim].max())\n",
    "    ax4.set_xlim(X_pred_points[:, dim].min(), X_pred_points[:, dim].max())\n",
    "    ax5.set_xlim(X_pred_points[:, dim].min(), X_pred_points[:, dim].max())\n",
    "    ax6.set_xlim(X_pred_points[:, dim].min(), X_pred_points[:, dim].max())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%y'))\n",
    "    ax1.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%y'))\n",
    "    ax2.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax3.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%y'))\n",
    "    ax3.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax4.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%y'))\n",
    "    ax4.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax5.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%y'))\n",
    "    ax5.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax6.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%y'))\n",
    "    ax6.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax2.locator_params(axis='y', nbins=5)\n",
    "    ax3.locator_params(axis='y', nbins=5)\n",
    "    ax4.locator_params(axis='y', nbins=5)\n",
    "    ax5.locator_params(axis='y', nbins=5)\n",
    "    ax6.locator_params(axis='y', nbins=5)\n",
    "    ax7.locator_params(axis='y', nbins=4)\n",
    "    ax.locator_params(axis='z', nbins=5)\n",
    "    #Makes me want to cry a little bit inside.\n",
    "    ax.zaxis._axinfo['label']['space_factor'] = 2.0\n",
    "    ax.yaxis._axinfo['label']['space_factor'] = 2.0\n",
    "    #fig_data.autofmt_xdate()\n",
    "    fig_latents.autofmt_xdate()\n",
    "    fig_mean.autofmt_xdate()\n",
    "    fig_var.autofmt_xdate()\n",
    "    fig_modes.autofmt_xdate()\n",
    "    fig_mean_var.autofmt_xdate()\n",
    "    fig3d.autofmt_xdate()\n",
    "    fig3d.tight_layout()\n",
    "    ax.autoscale(tight=True)\n",
    "    return fig_data, fig_latents, fig_mean, fig_var, fig_modes, fig3d, fig_mean_var\n",
    "\n",
    "from functools import partial\n",
    "m.save_plot_fs = partial(save_plot_fs, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.save_plot_fs(X_scale=X_scale, X_offset=X_offset, y_alpha=0.01, subsample=0.1)\n",
    "#fig_data, fig_latents, fig_mean, fig_var, fig_modes, fig3d = m.save_plot_fs(X_scale=X_scale, X_offset=X_offset, y_alpha=0.01, subsample=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib2tikz import save as tikz_save\n",
    "\n",
    "#Save all formats\n",
    "fig_data.savefig('labour_pdf.eps', rasterized=True, dpi=100, bbox_inches='tight')\n",
    "fig_data.savefig('labour_pdf.pdf', rasterized=True, dpi=100, bbox_inches='tight')\n",
    "\n",
    "fig3d.savefig('labour_3d.eps', rasterized=True, dpi=100, bbox_inches='tight', pad_inches=1.0)\n",
    "fig3d.savefig('labour_3d.pdf', rasterized=True, dpi=100, bbox_inches='tight', pad_inches=1.0)\n",
    "\n",
    "fig_latents.savefig('labour_latent.eps', rasterized=True, dpi=100, bbox_inches='tight')\n",
    "fig_latents.savefig('labour_latent.pdf', rasterized=True, dpi=100, bbox_inches='tight')\n",
    "\n",
    "tikz_save('labour_latent.tikz', fig_latents)\n",
    "tikz_save('labour_mean.tikz', fig_mean)\n",
    "tikz_save('labour_var.tikz', fig_var)\n",
    "tikz_save('labour_modes.tikz', fig_modes)\n",
    "tikz_save('labour_mean_vars.tikz', fig_mean_vars)\n",
    "#tikz_save('labour_pdf.tikz', fig_data)\n",
    "#tikz_save('labour_3d.tikz', fig3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
